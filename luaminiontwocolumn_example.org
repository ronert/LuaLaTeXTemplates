#+TITLE: Maß- und Wahrscheinlichkeitstheorie Übersicht
#+AUTHOR: Ronert Obst
#+EMAIL: ronert.obst@gmail.com
#+DATE: \today
#+LATEX_CMD: lualatex
#+LaTeX_CLASS: luaminiontwocolumn
#+LaTeX_CLASS_OPTIONS: [english]
* Erstes Kapitel
Duis auctor ligula et lorem fermentum commodo. Quisque commodo posuere nulla id gravida. Pellentesque pretium bibendum nisi, nec condimentum ante ullamcorper vitae. Vivamus non sapien mauris, quis tincidunt dui. Nunc cursus luctus felis in sagittis. Proin tincidunt blandit metus, eget convallis risus blandit ut. In hac habitasse platea dictumst. Sed molestie blandit nibh, vel laoreet nibh pretium accumsan. Proin id semper erat. Sed elementum congue mi quis dapibus. Nulla urna nibh, adipiscing sed sodales sit amet, sollicitudin convallis libero. Quisque eros lacus, auctor et egestas vel, gravida sit amet tortor.

Morbi sed nulla id orci sollicitudin aliquet ac at urna. In eu fringilla risus. Fusce urna ipsum, consequat non tempus nec, suscipit vitae ligula. Morbi volutpat, mauris sed adipiscing venenatis, risus sem porttitor nunc, vel pellentesque est leo vel enim. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Nam a arcu ipsum. Etiam convallis justo eu elit porttitor eget laoreet odio porta. Vivamus at sem enim, id ultrices turpis. Nulla ipsum nibh, convallis vitae lobortis sed, luctus eget augue. Mauris ut velit tortor. Etiam malesuada, nisl quis luctus placerat, dolor augue rutrum magna, id sollicitudin eros est ut neque. Vivamus accumsan, elit eu condimentum facilisis, dui risus commodo ipsum, eget lacinia nunc metus ac tellus. Maecenas felis mi, sollicitudin facilisis ornare quis, elementum bibendum metus. Ut fermentum vestibulum risus consectetur bibendum. Curabitur a diam luctus urna cursus viverra eu eu nisi.

Proin tempus rhoncus arcu sed sagittis. Fusce venenatis nisi eget felis commodo egestas. Ut aliquam, lectus dictum aliquam pulvinar, risus risus condimentum nulla, congue feugiat mauris lacus sit amet ligula. Fusce vel massa dolor. Nullam eleifend augue in enim fermentum elementum. Sed turpis magna, fringilla ut lobortis sit amet, luctus in risus. Vivamus sem nisi, mattis nec mollis vitae, blandit sit amet mauris. In metus magna, tempor at commodo ac, malesuada quis odio. Donec porttitor nunc ac justo dapibus in rutrum purus dictum. Mauris non posuere quam. Sed justo lacus, auctor sit amet placerat nec, auctor quis orci. Pellentesque et sapien vitae dolor malesuada mollis et posuere elit. Cras ut nisi mauris, id lacinia lectus. Curabitur mattis viverra urna vel aliquet. Praesent vitae mi dictum purus sodales auctor in id ante.

Suspendisse mi justo, eleifend vestibulum malesuada vel, luctus vel nulla. Aliquam consectetur nulla a eros suscipit tincidunt. Vestibulum quis adipiscing nunc. Vestibulum vitae diam vitae felis ultricies adipiscing eu non magna. Etiam fringilla arcu id ligula tincidunt semper. Ut quis fermentum erat. Aliquam hendrerit, augue quis malesuada dapibus, diam tellus posuere quam, non semper enim tellus sed velit. Nunc eros elit, placerat eget pretium at, pharetra hendrerit risus. Cras dapibus massa nunc. Proin sed lorem ligula. Donec malesuada odio sed eros malesuada eget commodo tellus cursus. Nam aliquam dictum laoreet. Donec sed lectus ligula.

Duis ultrices scelerisque porttitor. Curabitur rutrum, risus id interdum porta, lorem felis consectetur augue, commodo accumsan enim magna eu tortor. Maecenas varius pellentesque leo, et fermentum dolor dapibus nec. Pellentesque ipsum odio, pellentesque a feugiat nec, ullamcorper quis libero.

* Ungelöste Fragen
** WS11/12 Februar
*** Aufgabe 1
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Zeigen Sie, dass $\mathcal{P}(\mathbb{N})$ die kleinste $\sigma$-Algebra auf der Menge $\mathbb{N}$ der natürlichen Zahlen ist, die von allen endlichen Teilmengen von natürlichen Zahlen erzeugt ist.
\end{mdframed}

Sei $A_{i} \in \Natural$ die Menge aller endlichen Teilmengen von $\Natural$ mit $i\in\Natural$ Elementen, dann ist $\bigcup_{i=0}^{\infty}A_{i}$ die Menge aller endlichen Teilmengen von $\Natural$. Sei $\mathcal{E}\defeq A$ und $A_{i}^{C}=\Natural\setminus A_{i}$.
\[
\sigma(\mathcal{E})=\{\Omega,\emptyset,A,A^{C}\} = \mathcal{P}(\Natural)
\]
\begin{enumerate}[(i)]
\item $\Omega \in \mathcal{P}(\Natural)$
\item $A\in\mathcal{P}(\Natural) \;\Rightarrow\; A^{C}\in\mathcal{P}(\Natural)$
\item $(A_{i})_{i\in\Natural}\subset\mathcal{P}(\Natural)\;\Rightarrow\;\bigcup_{i\in\Natural}A_{i}\in\mathcal{P}(\Natural)$
\end{enumerate}
$\Rightarrow\;\;\sigma(\mathcal{E}) = \mathcal{P}(\Natural=\Omega)$ ist $\sigma$-Algebra (trivial da $\mathcal{P}(\Natural)$ per Definition eine $\sigma$-Algebra auf $\Omega$ ist).

Ist $\sigma(\mathcal{E})$ aber auch die kleinste $\sigma$-Algebra die $\mathcal{E}$ enthält?

Satz 2.11 aus Skript: $\sigma(\mathcal{E})$ von $\mathcal{E}$ erzeugte $\sigma$-Algebra\\
$\Rightarrow\; \sigma(\mathcal{E})$ ist kleinste $\sigma$-Algebra die $\mathcal{E}$ enthält.\\
$\Rightarrow\; \sigma(\mathcal{E})=\mathcal{P}(\Natural)$ ist kleinste $\sigma$-Algebra die von allen endlichen Teilmengen von $\Natural$ erzeugt wird.
** WS11/12 April alle
** One Thousand Exercises in Probability
- 7.9.5
* Sigma-Fields
** Definition
1) $\Omega \in \mathcal{A}$ 
2) $A\in \mathcal{A} \Rightarrow A^{C} \in \mathcal{A}$
3) $(A_{n}) \subset \mathcal{A} \Rightarrow \cup A_{n} \in \mathcal{A}$
   
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
The countable/co-countable $\sigma$-field. Let $\Omega = \mathbb{R}$ \\
$\ZZ: \mathcal{B}=\{A\subset\mathbb{R}: A\; \text{is countable}\} \cup \{A\subset\mathbb{R}:A^{C}\; \text{is countable}\}$ is a $\sigma$-field
\end{mdframed}
\begin{enumerate}[(M1)]
\item $\Omega \in \mathcal{B}\; (\text{since}\: \Omega^{C}=\emptyset\:\text{is countable})$
\item $A \in \mathcal{B}$ implies $A^{C}\in\mathcal{B}$
\item $A_{i}\in \mathcal{B}$ implies $\bigcap\limits_{i=1}^{\infty}A_{i}\in\mathcal{B}$ 
\end{enumerate}
** Intersections of Sigma-Algebras
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Man Beweise: Sei $\Omega$ eine Menge, sei $I$ eine Indexmenge und für jedes $i\in I$ sei $\mathcal{A}_i$ eine $\sigma$-Algebra auf $\Omega$. Dann ist auch
\[
\cap \mathcal{A}_{i} \defeq \{A\subset\Omega\given A\in\mathcal{A}_{i}\forall_{i}\in I\}
\]
eine $\sigma$-Algebra auf $\Omega$.
\end{mdframed}

1) $\Omega \in \mathcal{A}_{i} \forall_{i}\in I \Rightarrow \Omega \in\cap \mathcal{A}_{i}$ 
2) $A \in \cap\mathcal{A}_{i} \Rightarrow A \in \mathcal{A}_{i} \forall i \in I \Rightarrow A^{C} \in\cap\mathcal{A}_{i}$
3) $A_{n} \in\cap\mathcal{A}_{i}\forall n \in \mathbb{N} \Rightarrow A_{n} \in \mathcal{A}_{i}\forall_{i,n} \Rightarrow \cup A_{n} \in \mathcal{A}_{i} \Rightarrow \cup A_{n} \in \cap \mathcal{A}_{i}$

$\Rightarrow \cap\mathcal{A}_{i}$ ist $\sigma$-Algebra
** Minimal Sigma-Algebras
Let $\mathcal{C}$ be a collection of subsets of $\Omega$. The $\sigma$-field generated by $\mathcal{C}$, denoted $\sigma(\mathcal{C})$, is a \emph{minimal} $\sigma$-field satisfying
\begin{enumerate}[(a)]
\item $\sigma(\mathcal{C})\supset\mathcal{C}$
\item If $\mathcal{B}´$ is some other $\sigma$-field containing $\mathcal{C}$, then $\mathcal{B}´\supset\sigma(\mathcal{C})$
\end{enumerate}
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Given a class $\mathcal{C}$ of subsets of $\Omega$, there is a unique minimal $\sigma$-field containing $\mathcal{C}$.\\
\end{mdframed}

\textbf{Proof}: Let
\[
\aleph=\{\mathcal{B}:\mathcal{B}\:\text{is a}\: \sigma-\text{field},\: \mathcal{B}\supset\mathcal{C}\}
\]
be the set of all \sigma-fields containing $\mathcal{C}$. Then $\aleph\neq \emptyset$ since $\mathcal{P}(\Omega)\in\aleph$. Let
\[
\mathcal{B}^{\Game} = \bigcap\limits_{\mathcal{B}\in\aleph}\mathcal{B}.
\]
Since each class $\mathcal{B}\in\aleph$ is a \sigma-field, so is 
$\mathcal{B}^{\Game}$. Since $\mathcal{B}\in\aleph$ implies $\mathcal{B}\supset\mathcal{C}$, we have $\mathcal{B}^{\Game}\supset\mathcal{C}$. We claim $\mathcal{B}^{\Game}=\sigma(\mathcal{C})$. We checked $\mathcal{B}^{\Game}\supset\mathcal{C}$ and, for minimality, note that if $\mathcal{B}´$ is a \sigma-field such that $\mathcal{B}´\supset\mathcal{C}$, then $\mathcal{B}´\in\aleph$ and hence $\mathcal{B}^{\Game}\subset\mathcal{B}´$.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $\Omega=\{ 1,2,\ldots,7  \}$ and $\mathcal{E}=\{ \{1,2\}, \{6\}  \}$ then
\end{mdframed}
\[
\sigma{(\mathcal{E})} = \{ \emptyset, \{1,2\}, \{3,4,5,6,7\}, \{6\}, \{1,2,3,4,5,7\}, \{1,2,6\}, \{3,4,5,7\}, \Omega \}
\]

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $\Omega$ be set and $A\subset\Omega$. If $\mathcal{E}=\{A\}$ then
\end{mdframed}
\[
\sigma(\mathcal{E})=\{\emptyset,A,A^{C},\Omega\}
\]
** Inverse Maps
\textit{If $\Borel´$ is a $\sigma$-field of subsets of $\Omega´$, then $X^{-1}(\Borel´)$ is a $\sigma$-field of subsets of $\Omega$}\\
\textbf{Proof}:\\
\begin{enumerate}[(M1)]
\item Since $\Omega´ \in \Borel´$, we have
\[
X^{-1}(\Omega´)=\Omega\in X^{-1}(\Borel´)
\]
\item If $A´ \in \Borel´$, then $(A´)^{C} \in \Borel´$, and so if $X^{-1}(A´)\in X^{-1}(\Borel´)$ we have
\[
X^{-1}((A´)^{C})=(X^{-1}(A´))^{C}\in X^{-1}(\Borel´)
\]
\item If $X^{-1}(B´_{n})\in X^{-1}(\Borel´)$ then since $\bigcup\limits_{n}B´_{n}\in\Borel´$
\[
\bigcup\limits_{n}X^{-1}(B´_{n})=X^{-1}\left(\bigcup\limits_{n}B´_{n}\right)\in X^{-1}(\Borel´)
\]
\end{enumerate}

\textit{If $\mathcal{C}´$ is a class of subsets of $\Omega´$ then}
\[
X^{-1}(\sigma(\mathcal{C}´))=\sigma(X^{-1}(\mathcal{C}´))
\]

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
$\ZZ: f(\mathcal{A}_{1}): \{B\subset\mathcal{A}_{2}:f^{-1}(B)\in\mathcal{A}_{1}\}$ $\sigma$-Algebra auf $\Omega_2$
\end{mdframed}

\begin{enumerate}[(M1)] 
\item $\emptyset \in f(\mathcal{A}_{1}) \Rightarrow \Omega_{2} = \emptyset^{C} \in f(\mathcal{A}_{1})$
\item Sei $B \in f(\mathcal{A}_{2})$ \\ $f^{-1}(B) \in \mathcal{A}_{1} \Rightarrow (f^{-1}(B_{i}))^{C} \in \mathcal{A}_{1} \Rightarrow f^{-1}(B^{C}) \in \mathcal{A}_{1} \Rightarrow B^{C} \in f(\mathcal{A}_{1})$
\item Sei $B_{i} \in f(\mathcal{A}_{1})$ \\ $f^{-1}(B_{i}) \in \mathcal{A}_{1} \Rightarrow \bigcup\limits_{i\in\mathbb{N}} f^{-1}(B_{i}) \in \mathcal{A}_{1} \Rightarrow f^{-1}(\bigcup\limits_{i \in \mathbb{N}}) \in \mathcal{A}_{1} \Rightarrow \bigcup\limits_{i \in \mathbb{N}} B_{i} \in f(\mathcal{A}_{1})$
\end{enumerate}

* Measures
Let $\Algebra$ be a $\sigma$-field on $\Omega$. $\mu$ is a measure if
\[
\mu\: : \: \Algebra\: \rightarrow \: [0, \infty]
\]
such that
\begin{enumerate}[(M1)]
\item $\mu(\emptyset)=0$
\item For disjoint $A_{n}$
\[
\mu\left( \bigcup\limits_{n=1}^{\infty} A_{n} \right) = \Sum{\mu(A_{n})}{n,1,\infty}
\]
\end{enumerate}
** Probability Measures
*** Definition
\begin{enumerate}[(M1)]
\item $\mathbb{P}(A)\geq 0\: \forall \: A\in\mathcal{B}$
\item $\mathbb{P}$ is $\sigma$-additive for disjoint Events $A_{n}$
\[
\mathbb{P} \left( \bigcup\limits_{n=1}^{\infty} A_{n} \right)= \Sum{ \mathbb{P}(A_{n})}{n,1,\infty}.
\]
\item $\mathbb{P}(\Omega)=1$
\end{enumerate}
** Measurability
- Seien $(\Omega_{1},\mathcal{A}_{1}), (\Omega_{2},\mathcal{A}_{2})$ zwei Messräume. $X$ ist $\mathcal{A}_{1}-\mathcal{A}_{2}$-mb. falls
\begin{empheq}[box=\shadowbox*]{equation*}
X^{-1}(A)=\{\omega:X(\omega)\in A\} \in \mathcal{A}_{1} \forall \: A \in \mathcal{A}_{2}
\end{empheq}
- Das \textbf{Urbild} $X^{-1}(\mathcal{A}_{2})\defeq \{X^{-1}(A), A\in\mathcal{A}_{2}\}$ ist kleinste \sigma-Algebra bzgl. derer X mb. ist ($\sigma(X)\defeq X^{-1}(\mathcal{A}_{2})$)
- Sei $\mathcal{E}$ ein \textbf{Erzeuger} von $\mathcal{A}_{2}$, dann ist $X$ $\mathcal{A}_{1}-\mathcal{A}_{2}$-mb. falls $X^{-1}(E)\in\mathcal{A}_{1} \:\forall\: E \in \mathcal{E}$
** Image Measure
Sei $(\Omega, \Algebra, \mu)$ ein Maßraum, $(\Omega´,\Algebra´)$ ein Messraum und
\[
T:\; (\Omega,\Algebra) \rightarrow (\Omega´,\Algebra´)
\]
Das durch
\[
\mu´(A´) = \mu(T^{-1}(A´)) \; \forall A´\in\Algebra´
\]
definierte Maß $\mu´$ auf $(\Omega´,\Algebra´)$ heißt \textbf{Bildmaß} von $\mu$ unter $T$.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Sei $(\Omega, \mathcal{A}, \mu)$ der Maßraum mit $\Omega \defeq \mathbb{R}$ und der von allen abzählbaren Mengen erzeugten $\sigma$-Algebra $\mathcal{A}$, sowie $\mu(A)=0$ wenn $A$ abzählbar ist und $\mu(A)=1$ wenn $A^C$ abzählbar ist.

Für $\Omega´ \defeq \{0,1\}$ und $\mathcal{A}´ \defeq \mathcal{P}(\Omega´)$ wird die Abbildung $T: \Omega \rightarrow \Omega´$ definiert durch
\[
T(\omega) \defeq \begin{dcases*}
      0,     & falls $\omega$ rational \\
      1,  & falls $\omega$ irrational
    \end{dcases*}
\]
Man zeige, dass $T\:\mathcal{A} \rightarrow \mathcal{A}´$-messbar ist, und bestimmte das Bildmaß $T(\mu)$.
\end{mdframed}

\textbf{Antwort:}
$T$ ist messbar $\Leftrightarrow T^{-1}(A´) \in A \forall A´ \in \mathcal{A}$ \\
$\Omega´ = \{0,1\}\; \mathcal{A}´=\mathbb{P}(\Omega´)=\{\emptyset,\{0,1\},\{0\},\{1\}\}$

| $A´ \subset \mathcal{A}´$ \vert | $\emptyset$ | $0$          | $1$                             | $\{0,1\}$           |
|---------------------------------+-------------+--------------+---------------------------------+---------------------|
| $T^{-1}(A´)$ \vert              | $\emptyset$ | $\mathbb{Q}$ | $\mathbb{R}\setminus\mathbb{Q}$ | $\Omega=\mathbb{R}$ |

$\Rightarrow T\: \mathcal{A}-\mathcal{A}´$-mb

Bildmaß?

$\mu(T^{-1}(\emptyset))=\mu(\emptyset)=0$ \\
$\mu(T^{-1}(0))=\mu(\mathbb{Q})=0$ \\
$\mu(T^{-1}(1))=\mu(\mathbb{R}\setminus\mathbb{Q})=1$ \\
$\mu(T^{-1}(\{0,1\}))=\mu(\mathbb{R})=1$

* Integration and Expectation
** Expectation
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{X} = \Int{X}{\mathbb{P},\Omega} = \Int{xf(x)}{x,\Real}
\end{empheq}

\begin{equation}
\Expec{h(X)}=\Int{h(x)\,\mathbb{P_{X}}}{x,\Real} =
\begin{dcases*}
\Int{h(x)f(x)}{x,\Real} & im abs. stetigen Fall \\
\Sum{h(x_{k})\Prob{X=x_{k}}}{k,1,\infty} & im diskreten Fall
\end{dcases*}
\end{equation}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Erwartungswert von $e^{x}$ bei Normalverteilung
\end{mdframed}
$X\sim N(0,1),\;\;\;\Expec{e^{x}}$?
\begin{align*}
\Expec{e^{x}} & = \Int{e^{x}}{\mathbb{P},\Omega}  \\
& = \Int{e^{t} \mathbb{P}_{X}}{t,\Real} \\
& = \Int{e^{t}}{\lambda(t),\Real} \\
& = \Int{e^{t}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2}}}{t,\Real} \\
& = \Int{\frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}+t}}{t,\Real} \\
& = \Int{\frac{1}{\sqrt{2\pi}}*e^{\frac{-t^2+2t+1-1}{2}}}{t,\Real} \\
& = \Int{\frac{1}{\sqrt{2\pi}}e^{\frac{-(t^{2}-2t-1+1)}{2}}}{t,\Real} \\
& = \Int{\frac{1}{\sqrt{2\pi}}e^{\frac{-((t-1)^{2}-1)}{2}}}{t,\Real} \\
& = \Int{\frac{1}{\sqrt{2\pi}}e^{\frac{-(t-1)^{2}}{2}+\frac{1}{2}}}{t,\Real} \\
& = e^{\frac{1}{2}}\Int{\frac{1}{\sqrt{2\pi}}e^{\frac{-(t-1)^{2}}{2}}}{t,\Real}\;\;\; \sim N(1,1)=\text{Dichte} \\
& = e^{\frac{1}{2}}
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Varianz von Exponentialverteilter Zufallsvariable
\end{mdframed}
\label{expvar}
$X\sim \mathrm{Exp}(\lambda),\;\;\;\Var{X}$?
\[
\Expec{X}=\Int{t\lambda e^{-\lambda t}}{t,0,\infty} \overset{PI}{=} -e^{-\lambda t}t!\vert_{0}^{\infty} - \Int{1(-e^{-\lambda t})}{t,0,\infty} = 0 + \frac{1}{\lambda} = \frac{1}{\lambda}
\]
\begin{align*}
\Var{X}=\Expec{(X-\Expec{X})^{2}} & = \Int{(t-\frac{1}{\lambda})^{2}\lambda e^{-\lambda t}}{t,0,\infty} \\
& = \Int{t^{2}\lambda e^{-\lambda t}}{t,0,\infty} - \frac{2}{\lambda}\Int{t\lambda e^{-\lambda t}}{t,0,\infty} + \frac{1}{\lambda^{2}}\Int{\lambda e^{-\lambda t}}{t,0,\infty} \\
& \overset{PI}{=} -t^{2}e^{-\lambda t}\vert_{0}^{\infty} - \Int{2te^{-\lambda t}}{t,0,\infty} - \frac{2}{\lambda^{2}} + \frac{1}{\lambda^{2}} \\
& = 0 + \frac{2}{\lambda^{2}} - \frac{2}{\lambda^{2}} + \frac{1}{\lambda^{2}} = \frac{1}{\lambda^{2}}
\end{align*}
** Probability
\begin{empheq}[box=\shadowbox*]{equation}
\Prob{A} = \Int{}{\mathbb{P},A} = \Expec{\mathbb{1}_{A}}
\end{empheq}
** Distribution Function
\begin{empheq}[box=\shadowbox*]{equation}
F(x)=\Prob{(-\infty,x]}=\Prob{X\leq x}, \;\; x\in\mathbb{R}
\end{empheq}
** Monotone Convergence
If
\[
X_{n} \uparrow X
\]
then
\[
\Expec{X_{n}} \uparrow \Expec{X}
\]
and
\[
\Expec{\Sum{X_{i}}{i,1,\infty}} = \Sum{\Expec{X_{i}}}{i,1,\infty}
\]
** Dominated Convergence Theorem
If
\[
X_{n} \rightarrow X
\]
and there exists $Z\in\mathcal{L}_{1}$ such that
\[
\Abs{X_{n}} \leq Z
\]
then
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{X_{n}} \rightarrow \Expec{X} \; and \; \Expec{\Abs{X_{n}-X}} \rightarrow 0
\end{empheq}
** Integrable Random Variables
Define $\Expec{X}\defeq\Expec{X^{+}}-\Expec{X^{-}}$. The set of integrable random variables is denoted by $\mathcal{L}_{1}$:
\begin{empheq}[box=\shadowbox*]{equation}
\mathcal{L}_{1} = \{ \text{random variables}\: X\: : \: \Expec{\Abs{X}} < \infty \}
\end{empheq}
** Properties of Expectation
1) If $X$ is integrable, then \[\Prob{X=\pm\infty}=0\]
2) If $\Expec{X}$ exists, \[ \Expec{cX} = c\Expec{X}  \]
3) If $X\geq 0$ then $\Expec{X}\geq 0$ since $X=X^{+}$. If $X,Y \in \mathcal{L}_{1}$, and $X\leq Y$ then \[ \Expec{X} \leq \Expec{Y} \]
4) Suppose $\{X_n\}$ is a sequence of random variables such that $X_{n} \in \mathcal{L}_{1}$ for some $n$. If either \[ X_{n} \uparrow X  \] or \[ X_{n} \downarrow X  \] then \[ \Expec{X_{n}} \uparrow \Expec{X}  \] or \[ \Expec{X_{n}} \downarrow \Expec{X}  \]
5) If $X\in\mathcal{L}_{1}$, \[ \Abs{\Expec{X}} \leq \Expec{\Abs{X}}  \]
6) Variance and Covariance. If $X\in\mathcal{L}_{2}$ then 
\begin{align} 
\Var{X} & \defeq \Expec{X^{2}} - (\Expec{X})^{2} \\ 
\mathrm{Cov}(X,Y) & \defeq \Expec{XY} - \Expec{X} \Expec{Y} \\
\Var{\Sum{X_{i}}{i,1,n}} & = \Sum{\Var{X_{i}}}{i,1,n} + \Sum{\mathrm{Cov}(X_{i},X_{j})}{i,1,n}
\end{align}
** Fatou´s Lemma
If there exists $Z\in\mathcal{L}_{1}$ and $X_{n}\geq Z$ then
\begin{equation}
\Expec{\liminf\limits_{n\to\infty} X_{n}} \leq \liminf\limits_{n\to\infty}\Expec{X_{n}}
\end{equation}
and if $X_{n} \leq Z$ then
\begin{equation}
\limsup\limits_{n\to\infty}\Expec{X_{n}}\leq \Expec{\limsup\limits_{n\to\infty} X_{n}}
\end{equation}
** Fubini Theorem
Let $\mathbb{P} = \mathbb{P}_{1}\times\mathbb{P}_{2}$ be a product measure. If $X$ is $\Borel_{1}\times\Borel_2$ measurable and integrable with respect to $\mathbb{P}$ then
\begin{align}
\Int{X}{\mathbb{P},\Omega_{1}\times\Omega_{2}} & = \Int{\Int{X}{\mathbb{P}_{2}, \Omega_2}   }{\mathbb{P}_{1}, \Omega_{1}} \\
& = \Int{\Int{X}{\mathbb{P}_{1}, \Omega_1}   }{\mathbb{P}_{2}, \Omega_{2}}
\end{align}
** Tonelli
\[
\Int{f(\omega_{1},\ldots,\omega_{n})}{{\otimes_{i=1}^{n}\mu_{i}(\omega_{1},\ldots,\omega_{n})},\times^{n}_{i=1}\Omega_{i}}=
\int_{\Omega_{1}}\int_{\Omega_{2}}\cdots\int_{\Omega_{n}}f(\omega_{1},\ldots,\omega_{n})\,\mu_{n}(d\omega_{n})\ldots\mu_{1}(d\omega_{1})
\]
** Radon-Nikodym
Sei $(\Omega,\Algebra)$ ein Messraum, seien \mu und \nu zwei Maße auf $(\Omega, \Algebra)$ so dass
\[
d\nu=f\,d\mu
\]
für eine $\Algebra$-mb Funktion
\[
f\: : \: \Omega \rightarrow \Real \; \; \text{mit} \; \; f(w)\geq 0 \; \forall \omega\in\Omega
\]
Dann heisst $f$ \textbf{Dichte} oder Dichtefunktion von \nu bzgl. \mu.

Seien \mu und \nu Maße auf dem Maßraum $(\Omega,\Algebra)$, so dass für jedes $A \in \Algebra$ gilt
\[
\mu(A) = 0 \;\; \Rightarrow \;\; \nu(A)=0
\]
Dann sagt man \nu ist absolut stetig bzgl. \mu. Notation:
\[
\nu \ll \mu
\]

\textbf{Radon-Nikodym}: Seien \mu und \nu $\sigma$-endliche Maße auf dem Messraum $(\Omega,\Algebra)$. Dann sind folgende Aussagen äquivalent:
\begin{enumerate}[(i)]
\item $\nu$ besitzt eine Dichte bzgl. $\mu$
\item $\nu \ll \mu$
\end{enumerate}

Beispiel Normalverteilung
\begin{equation}
dN(\mu,\sigma^{2}) = f_{\mu,\sigma^{2}}d\lambda
\end{equation}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
\[
F(t) = \begin{dcases}
      0     & t < 0 \\
      \frac{t}{32}  & 0 \leq t < 1 \\
\frac{t^2}{16} & 1 \leq t < 2 \\
\frac{t}{8} + \frac{1}{4} & 2 \leq t < 4 \\
1 & t \geq 4
    \end{dcases}
\]
$\ZZ:$ Dichte bzgl. $\lambda + \delta_0 + \delta_1 + \delta_{2} + \delta_{4}$ \\ \\
\end{mdframed}
\textbf{Diskreter Teil}: Unstetigkeitsstellen \\
$\mathbb{P}[x_{i}]\geq 0 \; i=1,2,3 \;\; \alpha_{i} = \mathbb{P}[x_{i}], \; x_{1}=1, x_{2}=2, x_{3}=4$ \\ \\
\textbf{Absolut stetiger Teil}: $F(t)$ abs. stetig auf $\mathbb{R}\setminus\{1,2,4\}$ \\
d.h. $\mathbb{P}(B) = \Int{}{\mathbb{P},B} = \Int{f(x)}{\lambda,B} \; \forall \; B \in \mathcal{B}, \{1,2,4\}\notin B$ \\
$\mathbb{P}(B)=\mathbb{E}(\mathbb{1}_{B}) = \Int{\mathbb{1}_{B}}{\mathbb{P}} =\Int{}{\mathbb{P}, B}$ \\
$F(t) = \Int{f(t)}{t,-\infty,t} \Rightarrow F´(t)=f(t)$ \\
$\Rightarrow F´(t)=f(t)=\frac{1}{32}\mathbb{1}(0 < t < 1) + \frac{1}{8}t\mathbb{1}(1 < t < 2) + \frac{1}{8} \mathbb{1}(2 < f < 4)$ \\
\[
\Rightarrow \hat{f}(t)=
\begin{dcases}
f(t) & \forall t \in \mathbb{R}\setminus\{1,2,4\} \\
\alpha_{j} & \forall t = x_{j},\: j=1,2,3
\end{dcases}
\]
$\Rightarrow \hat{\mathbb{P}} \ll \mu$
** Transformationssatz für Dichten
Sei $f:\Real^{p}\rightarrow\Real,\; (x_{1},\ldots,x_{p})\mapsto f(x_{1},\ldots,x_{p})$ die $\lambda^{p}$-Dichte eines Wahrscheinlichkeitsmaßes $\mathbb{P}_{X}$. Seien $G, G´ \in \Borel^{\otimes p}$ offen und die Abbildung
\begin{align}
T:\; G &  \rightarrow G´ \\
(x_{1},\ldots,x_{p}) & \mapsto \left(T_{1}(x_{1},\ldots,x_{p}),\ldots,T_{p}(x_{1},\ldots,x_{p})\right)
\end{align}
bijektiv und samt $T^{-1}$ messbar und differenzierbar.\\
Dann gilt für die $\lambda^{p}$-Dichte $g$ von $T(\mathbb{P}_{X})$:
\begin{empheq}[box=\shadowbox*]{align}
g(y_1,\ldots,y_p) & = \Abs{\mathrm{det} \, J_{T^{-1}}(y_1,\ldots,y_p)} \cdot f\left( T^{-1}(y_1,\ldots,y_p)\right)\\
& = \Abs{\mathrm{det} \, J_{T}\left( T^{-1}(y_1,\ldots,y_p)\right)} \cdot f\left(T^{-1}(y_1,\ldots,y_p)\right)
\end{empheq}

Im \textbf{eindimensionalen} Fall vereinfacht sich die Dichtetransformationsformel zu
\begin{empheq}[box=\shadowbox*]{equation}
g(y)=\Abs{(T^{-1})´(y)}\cdot f\left(T^{-1}(y)\right)
\end{empheq}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Sei $X\thicksim\mathrm{Exp}$ mit der Dichte $f(x)=\lambda e^{-\lambda x}\mathbb{1}_{(0, \infty)}(x)$.\\
\end{mdframed}
Die Abbildung
\[
T:\;\; x \mapsto x^{2}
\]
ist bijektiv mit Umkehrfunktion
\[
y \mapsto \sqrt{y}
\]
Mit Ableitung
\[
\D{T^{-1}(y)}{y}= \frac{1}{2}y^{-\frac{1}{2}}
\]
Dann ist
\[
g(y) = \Abs{\frac{1}{2}y^{-\frac{1}{2}}}\cdot f(\sqrt{x})=\frac{1}{2}y^{-\frac{1}{2}}\cdot\lambda e^{-\lambda\sqrt{y}}
\]
für $y>0$.
** Convolutions
The Convolution $f=f_{1} \ast f_{2}$ of two densities $f_{1}$ and $f_{2}$ is defined by
\begin{empheq}[box=\shadowbox*]{equation}
f(z)=\Int{f_1(z-y)f_2(y)}{y,-\infty,+\infty}
\end{empheq}
* Conditional Expectation
\begin{empheq}[box=\shadowbox*]{align}
\Expec{Y \given X} & = \int y \cdot f_{Y \given X}(y\given x)dy = \int y \cdot\frac{f_{Y, X}(y,x)}{f_{X}(x)} dy = \int y \cdot\frac{f_{Y, X}(y,x)}{\int f_{Y, X}(y,x) dy} dy \\
\Expec{X \given B} & = \frac{1}{\Prob{B}}\Int{X}{\mathbb{P},B} = \frac{\Expec{X\cdot\mathbb{1}_B}}{\mathbb{P}(B)}  \\
\Expec{\psi(Y,X)\given X=x} & = \int_{\Omega_2}\int_{\Omega_1}\psi(y,x)\, \mathbb{P}^{Y \given X=x}dy\, \mathbb{P}^X dx
\end{empheq}
** Properties of Conditional Expectation
Sei $(\Omega, \Algebra, \mathbb{P})$ ein Wahrscheinlichkeitsraum und Seien
\[
f:\Omega\rightarrow\Real, \;\; f_{1}:\Omega\rightarrow\Real, \;\; f_{2}:\Omega\rightarrow\Real
\]
bzgl. $\mathbb{P}$ integrierbare Funktionen. Sei $\mathcal{C}$ eine Unter-$\sigma$-Algebra von $\Algebra$.\\
Dann gilt:
1) $\Expec{f \given \mathcal{C}} \in \mathcal{L}_{1}(\Omega, \Algebra, \mathbb{P})$
2) $\Expec{\Expec{f \given \mathcal{C}}} = \Expec{f}$
3) $f$ ist $\mathcal{C}$-messbar $\Rightarrow \Expec{f \given \mathcal{C}} = f \;\; \mathbb{P}$-f.s.
4) $f=g \mathbb{P}\text{-f.s.}\;\; \Rightarrow \;\; \Expec{f \given \mathcal{C}} = \Expec{g \given \mathcal{C}}\;\; \mathbb{P}$-f.s.
5) $f=\text{const}=\alpha \;\; \Rightarrow \;\; \Expec{f \given \mathcal{C}} = \alpha \mathbb{P}$-f.s.
6) Wenn $X_{i}$ iid sind, dann ist \[ \Expec{X\given\Sum{X_{i}}{i,1,n}} = \frac{\Sum{X_{i}}{i,1,n}}{n}  \] also z.b. $X,Y \sim \mathrm{Exp}(\lambda)$, dann ist \[ \Expec{X\given X+Y} \overset{iid}{=} \frac{X+Y}{2}  \]
7) Für $\alpha_{1}, \alpha_2 \in \Real$ ist $\Expec{\alpha_1 f_{1} + \alpha_2 f_{2} \given \mathcal{C}} = \alpha_1 \Expec{f_{1}\given \mathcal{C}}+\alpha_2\Expec{f_{2}\given\mathcal{C}}$
8) $f_{1} \leq f_{2} \mathbb{P}\text{-f.s.} \;\; \Rightarrow \;\; \Expec{f_{1} \given \mathcal{C}} \leq \Expec{f_{2}\given\mathcal{C}}$
9) $\Abs{\Expec{f\given\mathcal{C}}} \leq \Expec{ \Abs{f} \given \mathcal{C}  }$
10) \textbf{Monotone convergence}. If $X\in\mathcal{L}_{1},\:0\leq X_{n}\,\uparrow \,X$, then \[ \Expec{X_{n}\given \mathcal{C}} \,\uparrow\, \Expec{X\given \mathcal{C}} \]
11) Monotone convergence implies the \textbf{Fatou Lemma}. If $0\leq X_{n}\in\mathcal{L}_{1}$, then \[ \Expec{\liminf\limits_{n\to\infty}X_{n}\given \mathcal{C}} \leq \liminf\limits_{n\to\infty} \Expec{X_{n}\given\mathcal{C}} \] and while if $X_{n}\leq Z \in \mathcal{L}_{1}$, then \[ \Expec{\limsup\limits_{n\to\infty}X_{n}\given\mathcal{C}} \geq \limsup\limits_{n\to\infty} \Expec{X_{n}\given\mathcal{C}}  \]
12) Fatou implies \textbf{dominated convergence}. If $X_{n}\in\mathcal{L}_{1},\:\Abs{X_{n}}\leq Z\in\mathcal{L}_{1}$ and $X_{n}\rightarrow X_{\infty}$, then \[ \Expec{\lim\limits_{n\to\infty}X_{n}\given\mathcal{C}} \overset{a.s.}{=} \lim\limits_{n\to\infty}\Expec{X_{n}\given\mathcal{C}} \]
** Glättungseigenschaften
**** Glättungssatz
Sei $\mathcal{C}$ eine Unter-$\sigma$-Algebra von $\Algebra$. Sei $f,g\in\mathcal{L}_{1}$ so dass auch
\[
f\cdot g \in \mathcal{L}_{1}
\]
Sei $g$ außerdem $\mathcal{C}$-messbar.\\
Dann gillt
1) $\Expec{g\cdot f \given \mathcal{C}} = g\cdot\Expec{ f \given \mathcal{C} }$
2) $\Expec{g\cdot f} = \Expec{g\cdot \Expec{f \given \mathcal{C} } }$
**** Iteriertes Bedingen (Turmeigenschaft)
Seien $\mathcal{C}$ und $\mathcal{D}$ Unter-$\sigma$-Algebren von $\Algebra$, so dass
\[
\mathcal{C} \subset \mathcal{D} \subset \Algebra
\]
Sei 
\[
f\in\mathcal{L}_{1}
\]
Dann ist
\begin{equation}
\Expec{ \Expec{ f \given \mathcal{D}  } \given \mathcal{C} } = \Expec{ f \given \mathcal{C} }
\end{equation}
** Bedingte Dichten
\begin{empheq}[box=\shadowbox*]{equation}
f_{X \given Y}(x\given y) = \frac{f_{X, Y}(x,y)}{f_{Y}(y)}
\end{empheq}
** Bedingte Wahrscheinlichkeiten
\begin{empheq}[box=\shadowbox*]{align*}
\Prob{A} & = \Int{}{\mathbb{P},A} = \Expec{\mathbb{1}_{A}} \\
\Prob{A \given \mathcal{C}} & = \Expec{\mathbb{1}_A \given \mathcal{C}} \\
\Prob{A \given T} & = \Expec{\mathbb{1}_A \given T} \\
\Prob{A \given T=t} & = \Expec{\mathbb{1}_A \given T=t}  \\
\Prob{X \in A \given T=t} & = \Int{f_{X\given Y}(x\given y)}{x,A}
\end{empheq}
** Examples
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $X$ and $Y$ be jointly continious random variables with joint density
\[
f_{X,Y}(x,y) =
\begin{dcases}
e^{-x-y} & \text{if}\;y\geq 0 \\
0 & \text{otherwise}
\end{dcases}
\]
Compute $\Expec{X+Y \given X<Y}$:
\end{mdframed}

\begin{align*}
\Prob{X<Y} & = \Int{\Int{\left(f_{X,Y}(x,y)\right)}{y,x,\infty}}{x,-\infty,\infty} \\
& = \Int{\Int{e^{-x-y}}{y,x,\infty}}{x,0,\infty} \\
& = \Int{e^{-2x}}{x,0,\infty}=\frac{1}{2}
\end{align*}
Next,
\begin{align*}
\Expec{\mathbb{1}_{(X<Y)}(X+Y)} & = \Int{\Int{\left((x+y)f_{X,Y}(x,y)\right)}{y,x,\infty}}{x,-\infty,\infty} \\
& = \Int{\Int{(x+y)e^{-x-y}}{y,x,\infty}}{x,0,\infty} \\
& = \Int{(2x+1)e^{-2x}}{x,0,\infty} = 1
\end{align*}
It follows that
\[
\Expec{X+Y \given X<Y} = \frac{\Expec{\mathbb{1}_{(X<Y)}(X+Y)}}{\Prob{X<Y}}=\frac{1}{1/2}=2
\]

$X,Y$ haben gemeinsame Dichte $f_{X,Y}(x,y)=xe^{-x(y+1)}\cdot\mathbb{1}_{R^{2}}(x,y)$. Gesucht: $\Expec{Y\given X=x}$
\begin{align*}
f_{X}(x) & = \Int{f_{X,Y}(x,y)}{y} \\
& = \Int{x e^{ -x(y+1) } \cdot \mathbb{1}_{\Real^{2}_{+}}(x,y)}{y} \\
& = \Int{x e^{ -x(y+1) } \cdot \mathbb{1}_{\Real_{+}}(x,y)}{y,0,\infty} \\
& = e^{-x}\underbrace{\Int{xe^{-xy}\cdot\mathbb{1}_{\Real_{+}}(x)}{y,0,\infty}}_{\text{Dichte einer Exp. Vert.}=1} \\
& = e^{-x}\cdot\mathbb{1}_{R_{+}}(x)
\end{align*}
\begin{align*}
\Expec{Y\given X=x} & = \Int{y\cdot f_{Y\given X}(y\given x)}{x} \\
& = \Int{y\cdot \frac{f_{X,Y}(x,y)}{f_{X}(x)}}{x}
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Seien $X, Y$ Zufallsvariablen mit gemeinsamer Dichte $f_{X,Y}(x,y)=x(y-x)e^{-y}$ und $0\leq x\leq y < \infty$. \\
Geben Sie $\mathbb{E}[Y \given X]$ an. \\
Tip: (Merhfache) partielle Integration
\end{mdframed}

\label{aufgabe2}
$f_{Y\given X}(y \given x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}$

\begin{align*}
\Rightarrow\;\; f_{X}(x) & = \Int{f_{X,Y}(x,y)}{y,x,\infty} \\
& = \Int{x(y-x)e^{-y}}{y,x,\infty} \\
& = \Int{xye^{-y}}{y,x,\infty} - \Int{x^{2}e^{-y}}{y,x,\infty} \\
& = x[-e^{-y}(y+1)]_{x}^{\infty}-x^{2}[-e^{-y}]_{x}^{\infty} \\
& = x[0+e^{-x}(x+1)] - x^{2}[0+e^{-x}] \\
& = xe^{-x}(x+1)-x^{2}e^{-x} \\
& = x^{2}e^{-x}+xe^{-x}-x^{2}e^{-x} \\
& = xe^{-x}
\end{align*}

\begin{align*}
\Expec{Y\given X} & = \Int{yf_{Y\given X}(y\given x)}{y,x,\infty} \\
& = \Int{y \frac{x(y-x)e^{-y}}{xe^{-x}}}{y,x,\infty} \\
& = \Int{y(y-x)e^{x-y}}{y,x,\infty} \\
& = \Int{y^2 e^{x-y}-yxe^{x-y}}{y,x,\infty} \\
& = e^{x} \Int{y^2 e^{-y}}{y,x,\infty} -xe^{x}\Int{ye^{-y}}{y,x,\infty} \\
& = e^{x}[-y^{2}e^{-y}\vert_{x}^{\infty}+\Int{2ye^{-y}}{y,x,\infty}]-xe^{x}[-e^{-y}(y+1)]_{x}^{\infty} \\
& = e^{x} [x^{2}e^{-x}+2[-e^{-y}(y+1)]_{x}^{\infty}]-xe^{x}[e^{-x}(x+1)] \\
& = e^{x}x^{2}e^{-x}+2e^{-x}(x+1)e^{x}-xe^{x}e^{-x}(x+1) \\
& = 2 + x
\end{align*}
* Martingales
For integrable random variables $\{X_n,n\geq 0\}$ and $\sigma$-fields $\{\Borel_n, n\geq 0\}$ which are sub $\sigma$-fields of $\Borel$,  $\{(X_{n},\Borel_{n}),n\geq 0\}$ is a \textbf{martingale} if
\begin{enumerate}[(M1)]
\item Information accumulates, i.e. $\Algebra_{n} \subset \Algebra_{n+1}$
\item $X_{n}$ is adapted in the sense that for each $n$, $X_{n} \in\Borel_{n}$; that, $X_{n}$ is $\Borel_{n}$-measureable.
\item $\Expec{\Abs{X_{n}}}<\infty$
\item $\Expec{X_{n+1}\given \Borel_{n}} \overset{a.s.}{=} X_{n}$
\end{enumerate}

\begin{empheq}[box=\shadowbox*]{align}
\text{Sub-Martingal} & \leq \\
\text{Martingal bzgl.}\;\; (\Algebra_t)_{t\in T}:\;\Longleftrightarrow \; \forall s \leq t: \; X_s & = \Expec{X_t \given \Algebra_s},\;\;\mathbb{P}-f.s. \\
\text{Super-Martingal} & \geq
\end{empheq}
** Properties
1) $(X_t)_{t\in T}$ sei ein Martingal bzgl. $(\Algebra_{t})_{t\in T}$ mit $X_{t}\in\mathcal{L}_{p} \: \forall \: t\in T \: (1\leq p < \infty)$. Dann ist $\left(\Abs{X_{t}}^{p}\right)_{t\in T}$ ein Sub-Martingal bzgl. $(\Algebra_{t})_{t\in T}$
2) Für jedes $c\in\Real$ und Sub-Martingal $(X_{t})_{t\in T}$ ist auch $(\max\{c,X_{t}\})_{t\in T}$ ein Sub-Martingal bzgl. $(\Algebra_{t})_{t\in T}$. Insbesondere ist mit $c=0$ dann auch $(X_{t}^{+})_{t\in T}$ ein Sub-Martingal.
3) Ist $(X_{t})_{t\in T}$ ein Super-Martingal bzgl. $(\Algebra_{t})_{t\in T}$, so ist $(X_{t}^{-})_{t\in T}$ ein Sub-Martingal bzgl. $(\Algebra_{t})_{t\in T}$. Zur Erinnerung: $X_{t}^{-}\defeq -\min\{0,X_{t}\}$.
** Stopping Times
A mapping $\nu:\:\Omega\mapsto \bar{\mathbb{N}}$ is a stopping time if
\begin{equation}
[\nu = n] \in \Borel_{n},\;\; \forall n\in\Natural
\end{equation}
** Martingaldifferenzfolgen
Sei $\eta_{n}\in\mathcal{L}(\Omega,\Algebra,\mathbb{P}),\: n\in\Natural$, mit $\Algebra_{n}\defeq \sigma(\eta_{1},\ldots,\eta_{n})$ und $a\in\Real$ beliebig.\\
Definiere
\[
X_{1}\defeq \eta_{1}-a\;\text{und}\; X_{n+1}\defeq X_{n} + \eta_{n+1} - \Expec{\eta_{n+1}\given \Algebra_n}\;\; (n\geq 1)
\]
Dann gilt
\begin{align*}
\Expec{X_{n+1}\given \Algebra_n} & = \Expec{X_{n}\given \Algebra_n} + \Expec{\eta_{n+1}\given\Algebra_n} - \Expec{\Expec{\eta_{n+1}\given\Algebra_n}\given\Algebra_n} \\
& = X_{n} + \Expec{\eta_{n+1}\given\Algebra_n} - \Expec{\eta_{n+1}\given\Algebra_n} \\
& = X_{n}
\end{align*}
Das heißt, die Folge $(X_{n})_{n\in\Natural}$ bildet ein Martingal.\\

Ist umgekehrt $(X_{n})_{n\in\Natural}$ als Martingal vorausgesetzt und definiert man
\[
\eta_{1} \defeq X_{1}\;\;\;\;\;\;\eta_{n}\defeq X_{n}-X_{n-1}\;\;(n\geq 2)
\]
dann gilt
\begin{align*}
\Expec{\eta_{n+1}\given \eta_{1},\ldots,\eta_{n}} &= \Expec{X_{n+1}-X_{n}\given\eta_{1},\ldots,\eta_{n}}\\
&= \Expec{X_{n+1}-X_{n}\given X_{1},\ldots,X_{n}}\\
&= \Expec{X_{n+1}\given X_1,\ldots,X_{n}}-X_{n}\\
&= 0
\end{align*}

Daher ist eine Folge reeler integrierbarer Zufallsvariablen $(\eta_n)_{n\in\Natural}$ heißt \textbf{Martingaldifferenzfolge}, falls
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{\eta_{n+1}\given\eta_1,\ldots\eta_n} = 0 \;\;\;\; \mathbb{P}\text{-f.s.},\;\; \forall n\in\Natural
\end{empheq}
** Examples
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Seien $Z_{1},\ldots,Z_{n}$ unabhängig und identisch verteilt (iid) mit $Z_{i} \thicksim \mathcal{N}(0,1)$ und \\
$\mathcal{F}_{n}=\sigma(Z_{1},\ldots,Z_{n})$ eine Filtration. Ferner sei $X_{n}\defeq \Exp{\Sum{(Z_{i}-c)}{i,1,n}}, n \in \mathbb{N}, c \in \mathbb{R}$. \\
Für welche Werte $c$ ist $(X_{n})_{n \in \mathbb{N}}$ ein Martingal, Submartingal bzw. Supermartingal bzgl. $(F_{n})$? \\
Bitte begründen Sie Ihre Schritte kurz!
\end{mdframed}

\begin{itemize}
\item $X_{n}$ ist $\mathcal{F}_{n}$-mb. da Komposition aus $Z_{i}$ und $\mathcal{F}_{n}=\sigma(Z_{1},\ldots,Z_{n})$
\item $\mathcal{F}_{n}$ ist Filtration (Information komm hinzu) $\Rightarrow\;\mathcal{F}_{n}\subset\mathcal{F}_{n+1}\;\forall n$
\item $\Expec{\Abs{X_{n}}}<\infty$? (ist $Z_{n}$ integrierbar?)
\begin{align*}
\Expec{\Abs{X_{n}}} & = \Expec{\Exp{\Sum{Z_{i}-c}{i,1,n}}} \\
& = \Expec{\Prod{\Exp{Z_i-c}}{i,1,n}} \\
& \overset{\text{iid}}{=} \left(\Expec{\Exp{Z-c}}\right)^{n} \\
& = \left(\Int{\Exp{z-c}}{\mathbb{P}_{Z},\Real}\right)^{n} \\
& = \left( \Int{ \Exp{ z - c  }\frac{1}{\sqrt{2\pi}}\Exp{-\left(\frac{z^{2}}{2}\right)}}{z,\Real}  \right)^{n} \\
& = \left( \Int{\frac{1}{\sqrt{2\pi}} \Exp{ -\frac{z^{2}}{2} + z -c }}{z,\Real}  \right)^{n} \\
& = \left( \Int{\frac{1}{\sqrt{2\pi}} \Exp{ \frac{-z^{2}+2z - 2c}{2}  }}{z,\Real}  \right)^{n} \\
& = \left( \Int{\frac{1}{\sqrt{2\pi}} \Exp{ \frac{-z^{2}+2z-2c+1-1}{2}  }}{z,\Real}  \right)^{n} \\
& = \left( \Int{\frac{1}{\sqrt{2\pi}} \Exp{ \frac{-((z^{2}-1)^{2}-1+2c)}{2} }}{z,\Real}  \right)^{n} \\
& = \left( e^{\frac{1}{2}-c} \cdot \underbrace{\Int{\frac{1}{\sqrt{2\pi}} \Exp{ \frac{-(z-1)^2}{2} }}{z,\Real}}_{\sim N(1,1)=1}  \right)^{n} \\
& = (e^{\frac{1}{2}-c})^{n} \\
& = e^{n\left(\frac{1}{2}-c\right)} < \infty
\end{align*}
\item Martingaleigenschaft: $\Expec{X_{n+1}\given \mathcal{F}_{n}}\overset{\text{f.s.}}{=}X_{n}$?
\begin{align*}
\Expec{X_{n+1}\given \mathcal{F}_{n}} & = \Expec{X_{n}\cdot\Exp{Z_{n+1}-c}\given \mathcal{F}_{n}} \\
\left(X_{n}\: \text{ist} \: \mathcal{F}_{n}\text{-mb.}\right) \Rightarrow & = X_{n}\cdot\Expec{\Exp{Z_{n+1}-c}\given \mathcal{F}_{n}} \\
& \overset{\text{iid}}{=} X_{n}\cdot\Expec{\Exp{Z_{n+1}-c}} \\
& = X_{n}\cdot e^{\frac{1}{2}-c} \\
& = X_{n}\;\text{für}\; c=\frac{1}{2} \\ \\
\Rightarrow \; X_{n} \; \text{Martingal für} \; c & =\frac{1}{2} \\
X_{n} \;\text{Super-Martingal für} \; c & >\frac{1}{2} \\
X_{n} \;\text{Sub-Martingal für} \; c & < \frac{1}{2}
\end{align*}
\end{itemize}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
\textbf{Martingales and smoothing}. Suppose $X\in\mathcal{L}_{1}$ and $\{\Borel_{n},\,n\geq0\}$ is an increasing family of sub $\sigma$-fields of $\Borel$. Define for $n\geq 0$
\[
X_{n}\defeq \Expec{X\given\Borel_{n}}
\]
Then
\[
\{ (X_{n},\Borel_{n}), n \geq 0  \}
\]
is a martingale:
\end{mdframed}
\begin{align*}
\Expec{X_{n+1}\given\Borel_{n}} & = \Expec{\Expec{X\given\Borel_{n+1}}\given\Borel_n} \\
& = \Expec{X\given\Borel_n} \;\;\;\;\; \text{(smoothing)} \\
& = X_{n}
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
\textbf{Martingales and sums of independent random variables}. Suppose that $\{Z_{n},\,n\geq 0\}$ is an independent sequence of integrable random variables satisfying for $n\geq 0,\; \Expec{Z_{n}}=0$. Set $X_{0}=0,\;X_{n}=\Sum{Z_{i}}{i,1,n},\; n\geq 1$, and $\Borel_{n}\defeq \sigma(Z_{0},\ldots,Z_{n})$.
\end{mdframed}
Then $\{(X_{n},\,\Borel_n),\,n\geq 0  \}$ is a martingale since $\{(Z_{n},\,\Borel_n),\,n\geq 0  \}$ is a fair sequence.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Es sei $(X_{t})_{t\in\Natural}$ eine Folge von unabhängigen und identisch verteilten Zufallsvariablen mit $\Expec{X_{1}}=1$. Zeigen Sie, dass der stochastische Prozess $(Z_{t},t\in\Natural)$ mit
\[
Z_{t}=\Prod{X_{s}}{s,1,t}
\]
ein Martingal bezüglich der kanonischen Filtration $\sigma(X_{1},X_{2},\ldots)$ ist.
\end{mdframed}

Es gilt für jedes $t\in \Natural$:
\begin{align*}
\Expec{Z_{t+1} \given \Algebra_{t}} & = \Expec{ \Prod{X_{i}}{i,1,t+1} \given \sigma(X_{1},\ldots,X_{t}) } \\
& = \Expec{\Prod{X_{i}}{i,1,t}\given \sigma(X_{1},\ldots,X_{t})}\cdot\Expec{X_{t+1}\given \sigma(X_{1},\ldots,X_{t})} \\
& = \Prod{X_{i}}{i,1,t}\cdot\Expec{X_{t+1}} = \Prod{X_{i}}{i,1,t}=Z_{t}
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Es sei $(X_{t})_{t\in\Natural}$ eine Folge von unabhängigen und identisch verteilten Zufallsvariablen mit $\Expec{X_{1}}=0$ und $\Expec{X^2_{1}}=\sigma^2$. Weiter sei $S_t=\Sum{X_{s}}{s,1,t}$. Zeigen Sie, dass der stochastische Prozess $(Z_{t},t\in\Natural)$ mit
\[
Z_{t}=S^{2}_{t}-t\sigma^{2}
\]
ein Martingal bezüglich der kanonischen Filtration $\sigma(X_{1},X_{2},\ldots)$ ist.
\end{mdframed}
Es gilt für jedes $t\in \Natural$:
\begin{align*}
\Expec{Z_{t+1}\given\Algebra_t} & = \Expec{S_{t+1}^{2}-(t+1)\sigma^{2}\given\sigma{(X_{1},\ldots,X_{t})}} \\
& = \Expec{S_{t}^{2}+2S^{2}_{t}X_{t+1}+X^{2}_{t+1}\given\sigma(X_{1},\ldots,X_{t})} - (t+1)\sigma^{2} \\ 
& = S_{t}^{2} +\Expec{X_{t+1}^{2}} - (t+1)\sigma^{2} = S_{t}^{2} - t\sigma^{2} = Z_{t}
\end{align*}
* Convergence
** Almost Sure Convergence
We say that a statement about random elements hold \emph{almost surely} if there exists an event $A\in\Borel$ with $\Prob{A}=0$ such that the statement holds if $w\in A^{C}$.
\begin{empheq}[box=\shadowbox*]{equation}
\forall \epsilon > 0: \;\; \Prob{\limsup\limits_{n\to\infty}\Abs{X_n-X}>\epsilon}=0
\end{empheq}
Oder kurz
\[
X_{n} \overset{n\rightarrow\infty}{\longrightarrow}X \;\; \mathbb{P}-\text{f.s.}
\]
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $\{X_{r}:\geq 1\}$ be independent Poisson variables with respective parameters ${\lambda_{r}:r\geq 1}$. Show that $\Sum{X_{r}}{r,1,\infty}$ converges or diverges almost surely according as $\Sum{\lambda_r}{r,1,\infty}$
\end{mdframed}
The partial sum $S_{n}=\Sum{X_{r}}{r,1,n}$ is Poisson-distributed with parameters $\sigma_{n}=\Sum{\lambda_{r}}{r,1,n}$. For fixed $x$, the event $\{S_{n}\leq x\}$ is decreasing in $n$, whence by Lemma 1.3.5, if $\sigma_{n}\rightarrow \sigma < \infty$ and $x$ is non-negative integer.
\[
\Prob{\Sum{X_{r}\leq x}{r,1,\infty}} = \lim\limits_{n\to\infty} \Prob{S_{n}\leq x} = \Sum{\frac{e^{-\sigma}\sigma^{j}}{j!}}{j,0,x}
\]
Hence if $\sigma < \infty$, $\Sum{X_{r}}{r,1,\infty}$ converges to a Poisson random variable. On the other hand, if $\sigma_{n} \rightarrow \infty$ then $e^{-\sigma_{n}}\Sum{\frac{\sigma_{n}^{j}}{j!}}{j,0,x} \rightarrow 0$, giving that $\Prob{\Sum{X_{r}>x}{r,1,\infty}}=1$ for all $x$, and therefore the sum diverges with probability 1, as required.
*** Kolmogorov Convergence Criterion
If
\[
\Sum{\Var{X_{i}}}{i,1,\infty} < \infty
\]
then
\[
\Sum{\left(X_{i}-\Expec{X_{i}}\right)}{i,1,\infty}
\]
converges almost surely.
** Convergence in Probability
$X_{n} \overset{P}{\rightarrow} X$ if for $\forall \; \epsilon > 0$
\begin{empheq}[box=\shadowbox*]{equation}
\lim\limits_{n\to\infty}\Prob{\Abs{X_{n}-X} \geq \epsilon}=0
\end{empheq}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Sei $(X_{n})_{n\in\mathbb{N}}$ eine Folge unabhängiger Zufallsvariablen, welche $\text{Exp}(1)$-verteilt sind. \\
Zeigen Sie, dass $n^{\alpha}\cdot\min_{k \leq n} X_{k}$ stochastisch gegen Null konvergiert für alle $\alpha < 1, n\in\mathbb{N}$.
\end{mdframed}
\begin{align*}
\forall \epsilon > 0\;\;\;\lim\limits_{n\to\infty}\Prob{\Abs{n^{\alpha}\min_{k\leq n}X_{k}}\geq \epsilon} & = 0 \;\;\Longleftrightarrow\;\; n^{\alpha}\min_{k\leq n}X_{k}\; \overset{\mathbb{P}}{\longrightarrow} \; 0 \\
& = \lim\limits_{n\to\infty}\Prob{\min_{k\leq n}X_{k}\geq \frac{\epsilon}{n^{\alpha}}} \\
& = \lim\limits_{n\to\infty}\Prob{\bigcap\limits_{1\leq k\leq n}\{\omega:\: X_{k}(\omega)\}\geq \frac{\epsilon}{n^{\alpha}}} \\
& = \lim\limits_{n\to\infty}\Prod{\Prob{X_{k}\geq \frac{\epsilon}{n^{\alpha}}}}{k,1,n} \\
& \overset{\text{iid}}{=} \lim\limits_{n\to\infty} \left(\Prob{X_{1}\geq \frac{\epsilon}{n^{\alpha}}}\right)^{n} \\
& \overset{\text{Exp}(1)}{=} \lim\limits_{n\to\infty}\left(e^{-\frac{\epsilon}{n^{\alpha}}}\right)^{n} = 0
\end{align*}
** $L_{p}$ Convergence
$X\in\mathcal{L}_{p}$ means $\Expec{\Abs{X}^{p}} < \infty$. A sequence $\{ X_{n} \}$ of random variables converges in $\mathcal{L}_{p}$ to $X$, written
\[
X_{n} \overset{\mathcal{L}_{p}}{\rightarrow} X 
\]
if
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{\Abs{X_{n}-X}^{p}} \rightarrow 0
\end{empheq}
as $n\rightarrow\infty$.\\
It follows that if $X_{n}\overset{\mathcal{L}_{p}}{\rightarrow} X$ then $\Expec{\Abs{X_{n}^{p}}} \rightarrow \Expec{\Abs{X^{p}}}$

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Suppose $\{X_{n}\}$ is an iid sequence of random variables with $\Expec{X_{n}}=\mu$ and $\Var{X_{n}}=\sigma^{2}$. Then %
\[ %
\bar{X} = \Sum{ \frac{X_{i}}{n} }{i,1,n} \overset{\mathcal{L}_{2}}{\rightarrow}\mu, %
\]
\end{mdframed}
since
\begin{align*}
\left(\Expec{\frac{S_{n}}{n}-\mu}\right)^2 & = \frac{1}{n^{2}}\left(\Expec{S_{n}-n\mu}\right)^{2} \\
& = \frac{1}{n^{2}}\Var{S_{n}} \\
& = \frac{n\sigma^{2}}{n^{2}}\rightarrow 0.
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Suppose $X_{n} \overset{\mathcal{L}_{1}}{\rightarrow}X$. Show that $\Expec{X_{n}} \rightarrow \Expec{X}$. Is the converse true?
\end{mdframed}

We have that
\[
\Abs{\Expec{X_n}-\Expec{X}} = \Abs{\Expec{X_{n}-X}} \leq \Expec{\Abs{X_{n}-X}}\overset{n \rightarrow \infty}{\longrightarrow} 0
\]
The converse is clearly false. If each $X_{n}$ takes the values $\pm 1$, each with probability $\frac{1}{2}$, then $\Expec{X_{n}}=0$, but $\Expec{\Abs{X_{n}-0}}=1$.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
$\ZZ:$ $X_{n} \overset{\mathcal{L}_{2}}{\rightarrow} X \Rightarrow \Var{X_{n}}\rightarrow\Var{X}$
\end{mdframed}
$\Expec{X_{n}^{2}}\rightarrow\Expec{X^{2}}$ and $X_{n} \overset{\mathcal{L}_{1}}{\rightarrow}X$. Therefore $\Expec{X_{n}}\rightarrow\Expec{X}$. Thus $\Var{X_{n}}=\Expec{X^{2}_{n}}-\Expec{X_{n}}^{2}\rightarrow\Var{X}$.

** Convergence in Distribution (Weak Convergence)
\begin{empheq}[box=\shadowbox*]{align}
\lim\limits_{n\to\infty}\,\Expec{f\circ X_n}=\Expec{f\circ X} & \Longleftrightarrow \Int{f\circ X_n}{\mathbb{P}} \overset{n\rightarrow\infty}{\longrightarrow} \Int{f\circ X}{\mathbb{P}} \\
& \Longleftrightarrow \Int{f}{\mathbb{P}_{X_n}} \overset{n\rightarrow\infty}{\longrightarrow} \Int{f}{\mathbb{P}_X}
\end{empheq}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $\{X_n,\, n\geq 1\}$ be iid with common unit exponential distribution
\[
\Prob{X_n>x} = e^{-x},\;\;\;x>0
\]
Set $M_n=\vee_{i=1}^{n}X_i$ for $n\geq 1$. Then
\[
M_{n}-\Log{n} \Rightarrow Y,
\]
where
\begin{equation}\label{weakeg}
\Prob{Y\leq x} = \Exp{-e^{-x}},\;\;\;x\in\Real
\end{equation}
\end{mdframed}
To prove \cref{weakeg}, note that for $x\in \Real$,
\begin{align*}
\Prob{M_{n}-\Log{n}\leq{x}} & = \Prob{\bigcap\limits_{i=1}^{n}(X_{i}\leq x + \Log{n})} \\
& = (1-e^{-(x+\Log{n})})^{n}\\
& = \left(1-\frac{e^{-x}}{n}\right)^{n} \:\rightarrow\: \Exp{-e^{-x}}
\end{align*}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $X_{1},X_{2},\ldots,X_{n}$ be i.i.d. Cauchy. Show that $M_{n}= \max{X_{i}}$ is such that $\pi M_{n}/n$ converges in distribution, the limiting distribution function being given by $F(x)=e^{-1/x}$ if $x\geq 0$.
\end{mdframed}
We have that \[ \Prob{M_{m}\leq xn/\pi} = \left\{\frac{1}{2}+\frac{1}{\pi}\tan^{-1}\left(\frac{xn}{\pi}\right)\right\}^{n} = \left\{1-\frac{1}{\pi}\tan^{-1}\left(\frac{\pi}{xn}\right)\right\}^{n}  \]
if $x>0$, by elementary trigonometry. Now $\tan^{-1}y=y+o(y)$ as $y\rightarrow 0$, and therefore \[ \Prob{M_{m}\leq xn/\pi} = \left(1-\frac{1}{xn}+o(n^{-1})\right)^{n} \rightarrow e^{-1/x} \;\;\; \text{as}\, n\rightarrow\infty \]
*** Extreme Value Distributions
$\{X_n, n\geq 1\}$ idd with common distribution $F$. The Extreme observation among the first $n$ is
\[
M_{n}\defeq \vee_{i=1}^{n} X_{i}.
\]
Suppose there exist normalizing constants $a_{n}>0$ and $b_{n}\in\Real$ such that
\begin{equation}
F^{n}(a_{n}x+b_{n})=\Prob{\frac{M_{n}-b_{n}}{a_{n}}\leq x} \; \overset{D}{\rightarrow} \; G(x),
\end{equation}
where the limit distribution $G$ is proper and non-degenerate. Then $G$ is the type of one of the following extreme value distributions:
1) $\Phi_{\alpha}(x)=\Exp{-x^{-\alpha}},\;\;X>0,\;\;\alpha>0$,
2) $\Psi_{\alpha}(x)=\begin{dcases} \Exp{-(x)^{\alpha}}, & x<0,\;\;\alpha>0\\ 1 & x > 0,  \end{dcases}$
3) $\Lambda(x)=\Exp{-e^{-x}},\;\;x\in\Real$
The statistical significance is the following. The types of the three extreme value distributions can be united as a one parameter family indexed by shape parameter $\gamma\in\Real$:
\begin{equation}
G_{y}(x)=\Exp{-(1+\gamma x)^{-1/x}},\;\;1+\gamma x > 0
\end{equation}
where we interpret the case of $\gamma=0$ as
\[
G_{0}=\Exp{-e^{-x}}\;\; x\in\Real
\]
** Implications
\begin{empheq}[box=\shadowbox*]{equation}
\mathcal{L}_p-\text{Konvergenz} \; \Rightarrow \; \mathcal{L}_q-\text{Konvergenz}\:(q\leq p)
\Rightarrow \; \text{stochastische Konvergenz} \; \Rightarrow \; \text{schwache Konvergenz}
\end{empheq}
sowie
\begin{empheq}[box=\shadowbox*]{equation}
\text{fast sichere Konvergenz} \; \Rightarrow \; \text{stochastische Konvergenz}
\end{empheq}

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
$X_{i}$ i.i.d., $\Expec{X_{i}}=\mu$, $\Var{X_{i}}<\infty$. Show that
\[ \binom{n}{2}^{-1} \Sum{X_{i}X_{j}\overset{\mathbb{P}}{\rightarrow}\mu^{2}}{1\leq i \leq j \leq n},\;\; n\rightarrow\infty\]
\end{mdframed}
\[ \binom{n}{2}^{-1} \Sum{X_{i}X_{j}}{1\leq i \leq j \leq n} = \frac{n}{n-1} \left( \frac{1}{n} \Sum{X_{i}}{i,1,n}\right)^{2} - \frac{1}{n(n-1)}\Sum{X_{i}^{2}}{i,1,n} \]
Now $n^{-1}\Sum{X_{i}}{i,1,n}\overset{D}{\rightarrow}\mu$ by law of large numbers $\Rightarrow \; \; n^{-1}\Sum{X_i}{i,1,n}\overset{\mathbb{P}}{\rightarrow}\mu$ (see \cref{conv.impl}). It follows that $(n^{-1}\Sum{X_i}{i,1,n})^{2}\overset{\mathbb{P}}{\rightarrow}\mu^{2}$. Since if $c_{n}\rightarrow c$ and $X_{n}\overset{\mathbb{P}}{\rightarrow}X$ then $c_{n}X_{n}\overset{\mathbb{P}}{\rightarrow}cX$. So
\[ \frac{n}{n-1} \left( \frac{1}{n} \Sum{X_{i}}{i,1,n}\right)^{2} \overset{\mathbb{P}}{\rightarrow} \mu^{2}  \]
and
\[ \frac{1}{n(n-1)}\Sum{X_{i}^{2}}{i,1,n} \overset{\mathbb{P}}{\rightarrow} 0.  \]
The result follows from the fact that If $X_{n} \overset{\mathbb{P}}{\rightarrow} X$ and $Y_{n} \overset{\mathbb{P}}{\rightarrow} Y$ then $X_{n} + Y_{n} \overset{\mathbb{P}}{\rightarrow} X+Y$.

*** Converse Implications\label{conv.impl}
\begin{enumerate}[(a)]
\item If $X_{n} \overset{D}{\rightarrow} c$, where $c$ is constant, then $X_{n} \overset{\mathbb{P}}{\rightarrow} c$
\item If $X_{n} \overset{\mathbb{P}}{\rightarrow} X$ and $\Prob{\Abs{X_{n}}\leq k} = 1$ for all $n$ and some $k$, then $X_{n}\overset{\mathcal{L}_{p}}{\rightarrow} X$ for all $p \geq 1$
\item If $\Prob{\Abs{X_{n}-X}>\epsilon}$ satisfies $\Sum{\Prob{\Abs{X_{n}-X}>\epsilon}}{n}<\infty$ for all $\epsilon>0$, then $X_{n}\overset{\mathrm{a.s.}}{\rightarrow} X$
\end{enumerate}
*** Slutsky´s Theorem
\begin{empheq}[box=\shadowbox*]{equation}
X_n\:\overset{D}{\longrightarrow}\:X,\; A_n \:\overset{\mathbb{P}}{\longrightarrow}\: a \;\text{and} \; B_n\:\overset{\mathbb{P}}{\longrightarrow} \: b \;\; \Rightarrow \;\; A_n + B_n \cdot X_n \: \overset{D}{\longrightarrow} \: a+b*\cdot X
\end{empheq}
* Appendix
** Stammfunktionen
\begin{align*}
\Int{\frac{1}{x}}{x} & = \Log{\Abs{x}} + c \\
\Int{e^{x}}{x} & = e^{x} +c \\
\Int{e^{kx}}{x} & = \frac{1}{k}e^{kx}+c \\
\Int{a^{x}\Log{a}}{x} & = a^{x} + c \\
\Int{\Log{x}}{x} & = x\Log{x}-x \\
\Int{\Sin{x}}{x} & = -\Cos{x} + c \\
\Int{\Cos{x}}{x} & = \Sin{x} + c \\
\Int{e^{ax}}{x} & = \frac{1}{a}e^{ax} \\
\Int{xe^{ax}}{x} & = \frac{e^{ax}}{a^{2}}(ax-1)  \\
\Int{xe^{-ax}}{x} & = \frac{-e^{-ax}}{a^2}(ax+1)  \\
\Int{x^{2}e^{ax}}{x} & = \frac{e^{ax}}{a^{3}}(a^{2}x^{2}-2ax+2)  \\
\Int{x^{2}ae^{-ax}}{x,0,\infty} & = -x^{2}e^{-ax}\vert_{0}^{\infty} + \Int{2xe^{-ax}}{x,0,\infty} = 0 + \frac{2}{a^{2}}  \\
\Int{x^{n}e^{ax}}{x} & =  \frac{1}{a}x^{n}e^{ax}-\frac{n}{a}\Int{x^{n-1}e^{ax}}{x} \\
\Int{\frac{1}{1+e^{ax}}}{x} & = \frac{1}{a}\Log{\frac{e^{ax}}{1+e^{ax}}}  \\
\Int{\frac{1}{b+ce^{ax}}}{x} & = \frac{x}{b}-\frac{1}{ab}\Log{\Abs{b+ce^{ax}}}  \\
\Int{\frac{e^{ax}}{b+ce^{ax}}}{x} & = \frac{1}{ac}\Log{\Abs{b+ce^{ax}}}  \\
\end{align*}
*** Beispiele
- \Cref{aufgabe2}
- \Cref{expvar}
** Partielle Integration
\begin{empheq}[box=\shadowbox*]{equation}
\Int{f´(x)\cdot g(x)}{x,a,b} = [f(x) \cdot g(x)]_{a}^{b} - \Int{f(x) \cdot g´(x)}{x,a,b}
\end{empheq}
** Sets and Events
*** De Morgan
\begin{align*}
\left(\bigcup\limits_{i} A_{i}\right)^{C} &= \bigcap\limits_{i} A_{i}^{C} \\
\left(\bigcap\limits_{i} A_{i}\right)^{C} &= \bigcup\limits_{i} A_{i}^{C}
\end{align*}

*** Limits of Sets
- $\inf\limits_{k\geq n} A_{k} \defeq \bigcap\limits_{k=n}^{\infty}A_{k},\;\; \sup\limits_{k\geq n} A_{k} \defeq \bigcup\limits_{k=n}^{\infty}A_{k}$
- $\liminf\limits_{n\to\infty}A_{n}=\bigcup\limits_{n=1}^{\infty}\bigcap\limits_{k=n}^{\infty}A_{k}$
- $\limsup\limits_{n\to\infty}A_{n}=\bigcap\limits_{n=1}^{\infty}\bigcup\limits_{k=n}^{\infty}A_{k}$
- If $\liminf\limits_{n\to\infty}B_{n}=\limsup\limits_{n\to\infty}B_{n}=B$ then we say $B_{n}\rightarrow B$
- $\limsup\limits_{n\to\infty}A_{n}=[A_{n}\: i.o.]$
*** Borel-Cantelli Lemma
Let $\{A_{n}\}$ be any events. If
\[
\Sum{\Prob{A_n}}{n} < \infty
\]
then
\[
\Prob{A_{n} \: i.o.} = \Prob{\limsup\limits_{n\to\infty}A_{n}}=0
\]

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Let $X_{n}\sim\mathrm{Exp}(1)$ \[ \ZZ:\;\;\; \Prob{\limsup\limits_{n\to\infty} \frac{X_{n}}{\log n} =  1} = 1 \]
\end{mdframed}
Evidently \[\Prob{\frac{X_{n}}{\log n} \geq 1 + \epsilon} = \frac{1}{n^{1+\epsilon}},\;\;\;\text{for}\;\Abs{\epsilon}\leq 1  \]
By the Borel-Cantelli lemmas, the events $A_{n}=\{X_{n}/\log n \geq 1 + \epsilon\}$ occur a.s. infinitely often for $-1<\epsilon \leq 0$, and a.s. only finitely often for $\epsilon> 0$.
*** Borel Zero-One Law
If $\{A_n\}$ is a sequence of independent events, then
\[
\Prob{A_{n}\: i.o.} = 
\begin{dcases}
0, & \textit{iff}\: \Sum{\Prob{A_{n}}}{n} < \infty \\
1, & \textit{iff}\: \Sum{\Prob{A_{n}}}{n} = \infty
\end{dcases}
\]
** Inequalities
*** Markov
\begin{empheq}[box=\shadowbox*]{equation}
\mathbb{P}\left[ \Abs{X} \geq \lambda \right] \leq \frac{\mathbb{E}\left(\Abs{X}\right)}{\lambda}
\end{empheq}
*** Chebychev
\begin{empheq}[box=\shadowbox*]{equation}
\mathbb{P}\left[ \Abs{X-\mathbb{E}(X)} \geq \lambda \right] \leq \frac{\Var{X}}{\lambda^{2}}
\end{empheq}
*** Kolmogorov
\begin{empheq}[box=\shadowbox*]{equation}
\mathbb{P}\left[ \max_{1\leq k \leq n} \Abs{X_{k}} \geq \lambda \right] \leq \frac{\mathbb{V}(X_{n})}{\lambda^{2}} = \frac{1}{\lambda^{2}}\Sum{\Var{X_{k}}}{k,1,n}
\end{empheq}
*** Schwartz
$X,Y\in \mathcal{L}_{2}$ then
\begin{empheq}[box=\shadowbox*]{equation}
\Abs{\Expec{XY}}\leq \Expec{\Abs{XY}} \leq \sqrt{\Expec{X^{2}}\Expec{Y^{2}}}
\end{empheq}
*** Hölder
Suppose $p,q$ satisfy
\[
p>1,\; q>1,\; \frac{1}{p}+\frac{1}{q}=1
\]
and that
\[
\Expec{\Abs{X}^{p}}<\infty,\;\Expec{\Abs{X}^{q}}<\infty
\]
then
\begin{empheq}[box=\shadowbox*]{equation}
\Abs{\Expec{XY}}\leq \Expec{\Abs{XY}} \leq \left(\Expec{\Abs{X}^{p}}\right)^{1/p}\left(\Expec{\Abs{Y}^{q}}\right)^{1/q}
\end{empheq}
*** Minkowski
For $1\leq p < \infty$, assume $X,Y \in \mathcal{L}_{p}$. Then $X+Y\in\mathcal{L}_{p}$ and
\begin{empheq}[box=\shadowbox*]{equation}
\norm{X+Y}_{p}\leq \norm{X}_{p} + \norm{Y}_{p}
\end{empheq}
*** Jensen
Suppose $f:\Real\mapsto\Real$ is convex and $\Expec{\Abs{X}}<\infty$ and $\Expec{\Abs{f(X)}}<\infty$. Then
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{f(X)} \geq f(\Expec{X})
\end{empheq}
A special case is
\begin{empheq}[box=\shadowbox*]{equation}
\Expec{X^{2}} \geq (\Expec{X})^{2}
\end{empheq}
If $f$ is concave, the inequality reverses.
** Stochastics
*** Law of Large Numbers
\begin{empheq}[box=\shadowbox*]{equation}
\frac{1}{n}\Sum{X_{i}}{i,1,n} \rightarrow \mu
\end{empheq}
*** Central Limit Theorem
\begin{empheq}[box=\shadowbox*]{equation}
\mathbb{P}\left[ \frac{ \Sum{X_{i}-n\mu}{i,1,n} }{ \sigma\sqrt{n} }  \leq x \right] \rightarrow N(x) \defeq \Int{\frac{e^{-u^2/2}}{\sqrt{2\pi}}}{u,-\infty,x}
\end{empheq}

** Extema and Order Statistics
*** Minima
\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
Seien $X_{1},X_{2},\ldots$ iid auf $[0,1]$ Gleichverteilt. Gegen welche Verteilung konvergiert $n\cdot\min_{1\leq k \leq n} X_{k}$ schwach?
\end{mdframed}

\begin{align*}
\Prob{n\cdot\min_{1\leq k\leq n}<c} & = 1-\Prob{n\cdot\min_{1\leq k\leq n}\geq c} \\
& = 1-\Prob{\bigcap\limits_{1\leq k\leq n}\left\{\omega:X_{k}(\omega)\geq \frac{c}{n}\right\}} \\
& = 1-\left(\Prob{X\geq \frac{c}{n}}\right)^{n} \\
& = 1-\left(\Int{\mathbb{1}_{x\geq \frac{c}{n}}(x)\cdot \frac{1}{1-0}}{x}\right)^{n} \\
& = 1-\left(\Int{}{x,\frac{c}{n},1}\right)^{n} \\
& = 1 - \left(1-\frac{c}{n}\right)^{n} \\
\overset{\lim\limits_{n\to\infty}}{\longrightarrow} \: 1-e^{c}
\end{align*}

Konvergiert gegen ZV die Exp(1) verteilt ist.
*** Maxima
\begin{align*}
\Prob{\max_{1\leq k\leq n}X_{k}< c} & = \Prob{\bigcap_{1\leq k \leq n}\left\{ \omega: X_{k}(\omega) < c  \right\}} \\
& = \Prod{\Prob{X_{k}<c}}{k,1,n} \\
& = \left(\Prob{X_{1}<c}\right)^{n}
\end{align*}

